{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b2cd2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def load_dataset(path, max_len=20):\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    with open(path, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            inp, tgt = line.strip().split('\\t')\n",
    "            if len(inp) <= max_len and len(tgt) <= max_len:\n",
    "                inputs.append(inp)\n",
    "                targets.append(tgt)\n",
    "    return inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28b464bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, targets = load_dataset('data/zh_T9_dataset.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18d27b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer_input = Tokenizer(char_level=True, lower=False)\n",
    "tokenizer_input.fit_on_texts(inputs)\n",
    "\n",
    "tokenizer_output = Tokenizer(char_level=True, lower=False)\n",
    "tokenizer_output.fit_on_texts(targets)\n",
    "\n",
    "X = tokenizer_input.texts_to_sequences(inputs)\n",
    "y = tokenizer_output.texts_to_sequences(targets)\n",
    "\n",
    "X = pad_sequences(X, padding='post')\n",
    "y = pad_sequences(y, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "272467df",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "vocab_size_output = len(tokenizer_output.word_index) + 1\n",
    "y = to_categorical(y, num_classes=vocab_size_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de4b69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, TimeDistributed\n",
    "\n",
    "vocab_size_input = len(tokenizer_input.word_index) + 1\n",
    "max_input_len = X.shape[1]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size_input, output_dim=128, input_length=max_input_len))\n",
    "model.add(Bidirectional(LSTM(128, return_sequences=True)))\n",
    "model.add(TimeDistributed(Dense(vocab_size_output, activation='softmax')))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98af0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X, y, batch_size=64, epochs=10, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f01fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_top_k(digit_seq, k=5):\n",
    "    seq = tokenizer_input.texts_to_sequences([digit_seq])\n",
    "    seq = pad_sequences(seq, maxlen=max_input_len)\n",
    "\n",
    "    pred = model.predict(seq)[0]  # shape: (timesteps, vocab_size_output)\n",
    "    \n",
    "    top_k_chars_per_position = np.argsort(pred, axis=-1)[:, -k:][:, ::-1]\n",
    "\n",
    "    from itertools import product\n",
    "\n",
    "    candidates = []\n",
    "    for comb in product(*top_k_chars_per_position):\n",
    "        chars = [tokenizer_output.index_word.get(idx, '') for idx in comb if idx != 0]\n",
    "        candidate = ''.join(chars).strip()\n",
    "        if candidate:\n",
    "            candidates.append(candidate)\n",
    "\n",
    "    unique_candidates = list(dict.fromkeys(candidates))\n",
    "    return unique_candidates[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1050bd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = predict_top_k(\"4664\", k=5)\n",
    "\n",
    "for i, candidate in enumerate(results, 1):\n",
    "    print(f\"{i}. {candidate}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
